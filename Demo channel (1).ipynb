{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, face_pixels):\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    samples = np.expand_dims(face_pixels, axis=0)\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI_Retraining:\n",
    "    def __init__(self,path, rotation_range = 60, horizontal_flip = True):\n",
    "        from mtcnn import MTCNN\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        from keras.preprocessing.image import ImageDataGenerator\n",
    "        \n",
    "        self.datagen = ImageDataGenerator(rotation_range= rotation_range, horizontal_flip= horizontal_flip)\n",
    "        self.face_detector = MTCNN()\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.X_image = []\n",
    "        self.y_label = []\n",
    "        self.path = path\n",
    "        self.size = (160,160)\n",
    "    def load_image(self):\n",
    "        import cv2\n",
    "        import glob\n",
    "        try:\n",
    "            file_path = self.path + \"/*.*\"\n",
    "            for i in glob.glob(file_path):\n",
    "                img = cv2.imread(i)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                self.X_image.append(img)\n",
    "                label = i.split(\"/\")[-1]\n",
    "                label = label[:len(label)-9]\n",
    "                self.y_label.append(label)\n",
    "        except:\n",
    "            print(\"Invalid Path\")\n",
    "    def extract_face(self):\n",
    "        import cv2\n",
    "        X=[]\n",
    "        y=[]\n",
    "        try:\n",
    "            for i in range(len(self.X_image)):\n",
    "                img = self.X_image[i]\n",
    "                result = self.face_detector.detect_faces(img)\n",
    "                k=0\n",
    "                for k in range(len(result)):\n",
    "                    if(k >= 1):\n",
    "                        print(\"Required 1 person per image\")\n",
    "                    else:\n",
    "                        x1, y1, width, height = result[k]['box']\n",
    "                        x1, y1 = abs(x1), abs(y1)\n",
    "                        x2, y2 = x1+width, y1+height\n",
    "                        face = img[y1:y2, x1:x2]\n",
    "                        face = cv2.resize(face,self.size)\n",
    "                        X.append(face)\n",
    "                        y.append(self.y_label[i])\n",
    "            self.X_image = X\n",
    "            self.y_label = y\n",
    "        except Exception as e:\n",
    "            print(\"code failed in extract_face\")\n",
    "            print(e)\n",
    "    def face_augmentation(self, n = 7):\n",
    "        import glob\n",
    "        import os\n",
    "        import numpy as np\n",
    "        try:\n",
    "            if(os.path.isdir(\"Desktop/Kashmir Augmented Image\") == True):\n",
    "                for i in glob.glob(\"Desktop/Kashmir Augmented Image/*.*\"):\n",
    "                    os.remove(i)\n",
    "                os.rmdir(\"Desktop/Kashmir Augmented Image\")\n",
    "            self.augmentation_path = \"Desktop/Kashmir Augmented Image\" \n",
    "            os.mkdir(self.augmentation_path)\n",
    "            for i in range(len(self.X_image)):\n",
    "                img = self.X_image[i]\n",
    "                img = np.reshape(img,(1,160,160,3))\n",
    "                j=0\n",
    "                for batch in self.datagen.flow(img, batch_size=1, save_to_dir= self.augmentation_path, save_prefix = self.y_label[i], save_format = \"jpg\"):\n",
    "                    if(j >= n):\n",
    "                        break\n",
    "                    j+=1\n",
    "            return self\n",
    "        except Exception as e:\n",
    "            print(\"Code is failed in face_augmentation\")\n",
    "            print(e)\n",
    "            \n",
    "    def load_augmented_image(self):\n",
    "        import cv2\n",
    "        import glob\n",
    "        X = []\n",
    "        y_label = []\n",
    "        try:\n",
    "            for i in glob.glob(self.augmentation_path+\"/*.*\"):\n",
    "                img = cv2.imread(i)\n",
    "                img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "                X.append(img)\n",
    "                y = i.split(\"/\")[2]\n",
    "                y = y.split(\"_\")\n",
    "                y = y[0] + y[1]\n",
    "                y_label.append(y)\n",
    "            self.X_image = X\n",
    "            self.y_label = y_label\n",
    "        except Exception as e:\n",
    "            print(\"Code is failed in load_augmented_image\")\n",
    "            print(e)\n",
    "            \n",
    "    def features_to_numbers(self,path):\n",
    "        from keras.models import load_model\n",
    "        from keras.utils import np_utils\n",
    "        \n",
    "        self.facenet_model = load_model(path)\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(self.X_image)):\n",
    "            X.append(get_embedding(self.facenet_model, self.X_image[i]))\n",
    "        y = self.encoder.fit_transform(self.y_label)\n",
    "        self.no_classes = len(set(y))\n",
    "        y = np_utils.to_categorical(y, self.no_classes)\n",
    "        self.X_image = X\n",
    "        self.y_label = y\n",
    "    def training_NN(self, epochs = 10, path = \"Desktop/Kashmir Production/new_kashmir3.model\"):\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense\n",
    "        import numpy as np\n",
    "        self.nn_model = Sequential()\n",
    "        self.nn_model.add(Dense(self.no_classes*3, input_dim = 128 , activation = \"relu\"))\n",
    "        self.nn_model.add(Dense(self.no_classes*2, activation = \"relu\"))\n",
    "        self.nn_model.add(Dense(self.no_classes, activation = \"softmax\"))\n",
    "        self.nn_model.compile(loss = \"categorical_crossentropy\", optimizer= \"adam\", metrics=[\"accuracy\"])\n",
    "        print(self.nn_model.summary())\n",
    "        self.X_image = np.array(self.X_image)\n",
    "        self.y_label = np.array(self.y_label)\n",
    "        self.nn_model.fit(self.X_image, self.y_label, epochs = epochs)\n",
    "        self.nn_model.save(path)\n",
    "    def prediction(self,path,model_path = \"Desktop/Kashmir Production/new_kashmir3.model\"):\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        from keras.models import load_model\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        nn_model = load_model(model_path)\n",
    "        X_test = []\n",
    "        img = cv2.imread(path)\n",
    "        image = img\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        result = self.face_detector.detect_faces(img)\n",
    "        for k in range(len(result)):\n",
    "            x1, y1, width, height = result[k]['box']\n",
    "            x1, y1 = abs(x1), abs(y1)\n",
    "            x2, y2 = x1+width, y1+height\n",
    "            face = img[y1:y2, x1:x2]\n",
    "            face = cv2.resize(face,self.size)\n",
    "            X_test.append(face)\n",
    "        X_test = np.array(X_test)\n",
    "        for i in range(len(X_test)):\n",
    "            img = X_test[i]\n",
    "            img = get_embedding(self.facenet_model, img)\n",
    "            img = np.array(img)\n",
    "            img = np.reshape(img, (1,128))\n",
    "            y_pred = nn_model.predict(img)\n",
    "            y_pred = y_pred[0]\n",
    "            confidence = max(y_pred)\n",
    "            y_pred = np.argmax(y_pred)\n",
    "            y_pred = self.encoder.inverse_transform([y_pred])[0]\n",
    "            print(\"Name of Person = \",y_pred)\n",
    "            print(\"Accuracy = \",confidence*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 170 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc15880df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 174 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc186005400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 175 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc168f94e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 117)               15093     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 78)                9204      \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 39)                3081      \n",
      "=================================================================\n",
      "Total params: 27,378\n",
      "Trainable params: 27,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 3.7156 - accuracy: 0.0321\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3.4719 - accuracy: 0.1218\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.2963 - accuracy: 0.2244\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.1296 - accuracy: 0.3397\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.9502 - accuracy: 0.4551\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.7647 - accuracy: 0.5321\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.5646 - accuracy: 0.5769\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.3630 - accuracy: 0.6282\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.1603 - accuracy: 0.6603\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.9594 - accuracy: 0.7179\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.7671 - accuracy: 0.7756\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.5938 - accuracy: 0.7885\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.4358 - accuracy: 0.8141\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.2852 - accuracy: 0.8397\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.1534 - accuracy: 0.8718\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.0357 - accuracy: 0.8910\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.9277 - accuracy: 0.8846\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8317 - accuracy: 0.9038\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.7473 - accuracy: 0.9167\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.9359\n",
      "INFO:tensorflow:Assets written to: Desktop/Kashmir Production/new_kashmir3.model/assets\n",
      "Name of Person =  Abdul GaniBhajmasta\n",
      "Accuracy =  71.9317376613617 %\n"
     ]
    }
   ],
   "source": [
    "a = AI_Retraining(\"Desktop/kashmir_raw_dataset 2\")\n",
    "a.load_image()  #Loading Image\n",
    "a.extract_face()    #Face Extraction\n",
    "a.face_augmentation(3)    #Face Augmentation\n",
    "a.load_augmented_image()  # Loading Augmented Image\n",
    "a.features_to_numbers(\"Desktop/facenet_keras.h5/model/facenet_keras.h5\")      #Loading Facenet Model\n",
    "a.training_NN(20)             # Loading Neural Network \n",
    "a.prediction(path = \"Desktop/kashmir_raw_dataset 2/Abdul Gani_Bhajmasta_img1.jpg\")        #Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mahendi ShahChamalwas'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.encoder.inverse_transform([23])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
